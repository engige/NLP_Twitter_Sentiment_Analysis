{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and perform Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset to inspect its structure\n",
    "tweet_data = pd.read_csv('../data_file/tweet_sentiments.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of the following key columns:\n",
    "\n",
    "* **tweet_text:** The actual text of the Tweet.\n",
    "* **emotion_in_tweet_is_directed_at:** The product or brand mentioned in the Tweet (e.g., iPhone, iPad, Google).\n",
    "* **is_there_an_emotion_directed_at_a_brand_or_product:** The sentiment or emotion expressed in the Tweet (e.g., Positive emotion, Negative emotion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "tweet_text                                               1\n",
      "emotion_in_tweet_is_directed_at                       5802\n",
      "is_there_an_emotion_directed_at_a_brand_or_product       0\n",
      "dtype: int64\n",
      "\n",
      "Sentiment Distribution:\n",
      "No emotion toward brand or product    5389\n",
      "Positive emotion                      2978\n",
      "Negative emotion                       570\n",
      "I can't tell                           156\n",
      "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values and get a summary of the dataset\n",
    "missing_values = tweet_data.isnull().sum()\n",
    "\n",
    "# Check the distribution of sentiment classes\n",
    "sentiment_distribution = tweet_data['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()\n",
    "\n",
    "# Display the missing values and sentiment distribution\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(sentiment_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following observations:\n",
    "\n",
    "1. There is one missing value in the `tweet_text` column and a large number (5802) in the `emotion_in_tweet_is_directed_at column`, which is not crucial for sentiment classification as our primary target is the sentiment.\n",
    "\n",
    "2. The sentiment distribution shows a significant imbalance, with:\n",
    "   * **5389** instances labeled as \"No emotion toward brand or product.\"\n",
    "   * **2978** labeled as \"Positive emotion.\"\n",
    "   * **570** labeled as \"Negative emotion.\"\n",
    "   * **156** instances labeled as \"I can't tell.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing tweet_text and drop the column 'emotion_in_tweet_is_directed_at' as it is not necessary for sentiment analysis\n",
    "cleaned_tweet_data = tweet_data.dropna(subset=['tweet_text']).drop(columns=['emotion_in_tweet_is_directed_at'])\n",
    "\n",
    "# Display the cleaned dataset for further inspection\n",
    "cleaned_tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                            0\n",
       "is_there_an_emotion_directed_at_a_brand_or_product    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-checking that there are no missing values\n",
    "missing_values = cleaned_tweet_data.isnull().sum()\n",
    "\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been successfully cleaned by removing the missing entries from the `tweet_text` column and dropping the irrelevant `emotion_in_tweet_is_directed_at` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\engig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\engig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\engig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>wesley g iphone hr tweeting riseaustin dead ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>jessedee know fludapp awesome ipadiphone app y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>swonderlin wait ipad also sale sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>sxsw hope year festival isnt crashy year iphon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>sxtxstate great stuff fri sxsw marissa mayer g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  wesley g iphone hr tweeting riseaustin dead ne...  \n",
       "1  jessedee know fludapp awesome ipadiphone app y...  \n",
       "2                swonderlin wait ipad also sale sxsw  \n",
       "3  sxsw hope year festival isnt crashy year iphon...  \n",
       "4  sxtxstate great stuff fri sxsw marissa mayer g...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for text preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing function to the 'tweet_text' column\n",
    "cleaned_tweet_data['cleaned_text'] = cleaned_tweet_data['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "cleaned_tweet_data[['tweet_text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cleaned_text` column now contains preprocessed Tweet text, which has been:\n",
    "\n",
    "* Converted to lowercase.\n",
    "* Stripped of punctuation and special characters.\n",
    "* Tokenized, with stop words removed.\n",
    "* Lemmatized to reduce words to their base form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Feature Engineering\n",
    "\n",
    "Convert the cleaned text into numerical features using Term Frequency-Inverse Document Frequency (TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9092, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the TF-IDF Vectorizer with a maximum of 5000 features and stop words removed\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the cleaned text data into TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_tweet_data['cleaned_text'])\n",
    "\n",
    "# Display the shape of the resulting TF-IDF matrix\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matrix contains 9,092 rows (one for each Tweet) and 5,000 TF-IDF features (words or terms) based on the cleaned text data. Each cell in the matrix represents the TF-IDF score for a specific word in a specific Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aapl  aaron  aarpbulletin   ab  abacus  abba  abc  ability  able  abnormal\n",
      "0   0.0    0.0           0.0  0.0     0.0   0.0  0.0      0.0   0.0       0.0\n",
      "1   0.0    0.0           0.0  0.0     0.0   0.0  0.0      0.0   0.0       0.0\n",
      "2   0.0    0.0           0.0  0.0     0.0   0.0  0.0      0.0   0.0       0.0\n",
      "3   0.0    0.0           0.0  0.0     0.0   0.0  0.0      0.0   0.0       0.0\n",
      "4   0.0    0.0           0.0  0.0     0.0   0.0  0.0      0.0   0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the sparse TF-IDF matrix to a dense array\n",
    "tfidf_sample = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first 5 rows and first 10 columns of the TF-IDF matrix\n",
    "print(tfidf_sample.iloc[:5, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above displays a small portion of the matrix, showing how the first 5 Tweets relate to the first 10 words in the vocabulary. The values represent the TF-IDF scores for each word in each Tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binary Classification\n",
    "\n",
    "Converted the sentiment labels into a binary classification problem (positive vs. negative). Trained binary classifiers models like Logistic Regression, Random Forest and SVM using the TF-IDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1(a). Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8394366197183099\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      0.68      0.57       114\n",
      "    Positive       0.93      0.87      0.90       596\n",
      "\n",
      "    accuracy                           0.84       710\n",
      "   macro avg       0.72      0.77      0.74       710\n",
      "weighted avg       0.86      0.84      0.85       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Convert sentiment labels into a binary classification task (positive vs. negative)\n",
    "# Exclude 'No emotion' and 'I can't tell' classes for this binary task\n",
    "binary_data = cleaned_tweet_data[cleaned_tweet_data['is_there_an_emotion_directed_at_a_brand_or_product'].isin(['Positive emotion', 'Negative emotion'])]\n",
    "\n",
    "# Re-apply the TF-IDF transformation on the filtered data\n",
    "X_tfidf_binary = tfidf_vectorizer.transform(binary_data['cleaned_text'])\n",
    "\n",
    "# Prepare target variable (1 for positive, 0 for negative)\n",
    "y_binary = binary_data['is_there_an_emotion_directed_at_a_brand_or_product'].apply(lambda x: 1 if x == 'Positive emotion' else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "# Train a Logistic Regression model as a baseline\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_log = accuracy_score(y_test, y_pred)\n",
    "report_log = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "\n",
    "# Display the evaluation scores\n",
    "print(f\"Accuracy: {accuracy_log}\")\n",
    "print(\"Classification Report:\\n\", report_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the baseline logistic regression binary classification task (positive vs. negative sentiment):\n",
    "\n",
    "* **Accuracy:** 84%\n",
    "\n",
    "* **Negative Class (Precision: 0.50, Recall: 0.68, F1-score: 0.57):** The model struggles with correctly identifying negative sentiment, which is expected due to the class imbalance.\n",
    "\n",
    "* **Positive Class (Precision: 0.93, Recall: 0.87, F1-score: 0.90):** The model performs much better with positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1(b). Logistic Regression with SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after SMOTE: 0.8492957746478873\n",
      "Classification Report after SMOTE:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.53      0.61      0.57       114\n",
      "    Positive       0.92      0.89      0.91       596\n",
      "\n",
      "    accuracy                           0.85       710\n",
      "   macro avg       0.73      0.75      0.74       710\n",
      "weighted avg       0.86      0.85      0.85       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Convert sentiment labels into a binary classification task (positive vs. negative)\n",
    "binary_data = cleaned_tweet_data[cleaned_tweet_data['is_there_an_emotion_directed_at_a_brand_or_product'].isin(['Positive emotion', 'Negative emotion'])]\n",
    "\n",
    "# Re-apply the TF-IDF transformation on the filtered data\n",
    "X_tfidf_binary = tfidf_vectorizer.transform(binary_data['cleaned_text'])\n",
    "\n",
    "# Prepare target variable (1 for positive, 0 for negative)\n",
    "y_binary = binary_data['is_there_an_emotion_directed_at_a_brand_or_product'].apply(lambda x: 1 if x == 'Positive emotion' else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "# Apply SMOTE to the training data to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Logistic Regression model with SMOTE-applied data\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "logreg.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_smote = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_log_smote = accuracy_score(y_test, y_pred_smote)\n",
    "report_log_smote = classification_report(y_test, y_pred_smote, target_names=['Negative', 'Positive'])\n",
    "\n",
    "# Display the evaluation scores\n",
    "print(f\"Accuracy after SMOTE: {accuracy_log_smote}\")\n",
    "print(\"Classification Report after SMOTE:\\n\", report_log_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the binary classification task for logistic regression model with SMOTE applied:\n",
    "\n",
    "* **Accuracy:** 85%\n",
    "\n",
    "* **Negative Class (Precision: 0.53, Recall: 0.61, F1-score: 0.57):** The model shows some improvement in precision for the negative sentiment (minority class) after applying SMOTE. However, recall slightly dropped compared to the baseline, leading to an unchanged \n",
    "F1-score. This suggests that while the model is better at identifying true negatives, it still struggles with correctly classifying all the negative cases.\n",
    "\n",
    "* **Positive Class (Precision: 0.92, Recall: 0.89, F1-score: 0.91):** The performance for the positive class is still strong but has slightly reduced from the baseline model, which is likely a trade-off introduced by balancing the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1(c). Tuned Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Accuracy after Tuning: 0.8774647887323944\n",
      "Classification Report after Tuning:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.61      0.61       114\n",
      "    Positive       0.92      0.93      0.93       596\n",
      "\n",
      "    accuracy                           0.88       710\n",
      "   macro avg       0.77      0.77      0.77       710\n",
      "weighted avg       0.88      0.88      0.88       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'penalty': ['l2'],  # Penalty norm (L1 norm is typically supported with 'liblinear' or 'saga')\n",
    "    'solver': ['liblinear', 'saga']  # Solvers compatible with penalty\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Set up GridSearchCV with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV to the SMOTE-balanced training data\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Display the best hyperparameters found\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Train the best model found by GridSearchCV\n",
    "best_logreg = grid_search.best_estimator_\n",
    "y_pred_best = best_logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_best)\n",
    "report_tuned = classification_report(y_test, y_pred_best, target_names=['Negative', 'Positive'])\n",
    "\n",
    "# Display the evaluation scores\n",
    "print(f\"Accuracy after Tuning: {accuracy_tuned}\")\n",
    "print(\"Classification Report after Tuning:\\n\", report_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the binary classification task with tuned (hyperparameter optimization) logistic regression model: \n",
    "\n",
    "* **Accuracy: 88%**\n",
    "\n",
    "* **Negative Class (Precision: 0.62, Recall: 0.61, F1-score: 0.61):** Precision has improved compared to the SMOTE-only model (0.53 → 0.62), and recall remains the same at 0.61. This indicates that the model has become better at correctly identifying true negatives while maintaining its ability to capture the negative class.\n",
    "\n",
    "* **Positive Class (Precision: 0.92, Recall: 0.93, F1-score: 0.93):** The performance for the positive class remains strong, with a slight improvement in recall compared to the SMOTE-only model (0.89 → 0.93), resulting in a slightly better F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1(d). Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest: 0.8901408450704226\n",
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.40      0.54       114\n",
      "    Positive       0.90      0.98      0.94       596\n",
      "\n",
      "    accuracy                           0.89       710\n",
      "   macro avg       0.86      0.69      0.74       710\n",
      "weighted avg       0.88      0.89      0.87       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100)\n",
    "\n",
    "# Train the model on the SMOTE-balanced data\n",
    "rf_classifier.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf, target_names=['Negative', 'Positive'])\n",
    "\n",
    "# Display the evaluation scores\n",
    "print(f\"Accuracy for Random Forest: {accuracy_rf}\")\n",
    "print(\"Classification Report for Random Forest:\\n\", report_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the binary classification task with Random Forest classifier:\n",
    "\n",
    "* **Accuracy: 89%**\n",
    "\n",
    "* **Negative Class (Precision: 0.82, Recall: 0.40, F1-score: 0.54):** The precision for the negative class has significantly improved compared to the tuned Logistic Regression model (0.62 → 0.82). However, recall has dropped noticeably (0.61 → 0.40), resulting in a lower F1-score for the negative class (0.61 → 0.54). This suggests that while the model is very good at identifying negative sentiment when it predicts it, it is missing a substantial portion of actual negative cases.\n",
    "\n",
    "* **Positive Class (Precision: 0.90, Recall: 0.98, F1-score: 0.94):** The Random Forest model performs exceptionally well for the positive class, with very high recall (0.98) and a strong F1-score (0.94), slightly outperforming the tuned Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1(e). Tuned Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Best Parameters: {'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Accuracy after Tuning: 0.8873239436619719\n",
      "Classification Report after Tuning:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.38      0.52       114\n",
      "    Positive       0.89      0.98      0.94       596\n",
      "\n",
      "    accuracy                           0.89       710\n",
      "   macro avg       0.86      0.68      0.73       710\n",
      "weighted avg       0.88      0.89      0.87       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],    # Minimum samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt']   # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Set up GridSearchCV with 3-fold cross-validation\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV to the SMOTE-balanced training data\n",
    "grid_search_rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Display the best hyperparameters found\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "print(f\"Best Parameters: {best_params_rf}\")\n",
    "\n",
    "# Train the best model found by GridSearchCV\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "accuracy_tuned_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "report_tuned_rf = classification_report(y_test, y_pred_best_rf, target_names=['Negative', 'Positive'])\n",
    "\n",
    "# Display the evaluation scores\n",
    "print(f\"Accuracy after Tuning: {accuracy_tuned_rf}\")\n",
    "print(\"Classification Report after Tuning:\\n\", report_tuned_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the binary classification task with tuned Random Forest model:\n",
    "\n",
    "* **Accuracy: 89%**\n",
    "\n",
    "* **Negative Class (Precision: 0.83, Recall: 0.38, F1-score: 0.52):** The precision for the negative class has improved (0.82 → 0.83) compared to the untuned model, but recall remains low at 0.38 (previously 0.40). The F1-score for the negative class is still relatively low (0.52), indicating that the model struggles to capture the majority of negative cases.\n",
    "\n",
    "* **Positive Class (Precision: 0.89, Recall: 0.98, F1-score: 0.94):** The performance for the positive class remains strong, with high recall (0.98) and F1-score (0.94). The slight decrease in precision (0.90 → 0.89) has not significantly impacted the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1(f). Tuned Random Forest Classifier (RandomSearchCV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None}\n",
      "Accuracy after Randomized Search Tuning: 0.8732394366197183\n",
      "Classification Report after Randomized Search Tuning:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.68      0.39      0.50       114\n",
      "    Positive       0.89      0.96      0.93       596\n",
      "\n",
      "    accuracy                           0.87       710\n",
      "   macro avg       0.79      0.68      0.71       710\n",
      "weighted avg       0.86      0.87      0.86       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV with 3-fold cross-validation\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist,\n",
    "                                      n_iter=10, cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the RandomizedSearchCV to the SMOTE-balanced training data\n",
    "random_search_rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Display the best hyperparameters found\n",
    "best_params_rf_random = random_search_rf.best_params_\n",
    "print(f\"Best Parameters: {best_params_rf_random}\")\n",
    "\n",
    "# Train the best model found by RandomizedSearchCV\n",
    "best_rf_random = random_search_rf.best_estimator_\n",
    "y_pred_best_rf_random = best_rf_random.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "accuracy_tuned_rf_random = accuracy_score(y_test, y_pred_best_rf_random)\n",
    "report_tuned_rf_random = classification_report(y_test, y_pred_best_rf_random, target_names=['Negative', 'Positive'])\n",
    "\n",
    "# Display the evaluation scores\n",
    "print(f\"Accuracy after Randomized Search Tuning: {accuracy_tuned_rf_random}\")\n",
    "print(\"Classification Report after Randomized Search Tuning:\\n\", report_tuned_rf_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
